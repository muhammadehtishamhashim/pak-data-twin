# LSTM Forecasting Models

## ðŸ¤– Overview

This document details the Long Short-Term Memory (LSTM) neural network models used for forecasting economic indicators in the Pakistan Data Twin Dashboard.

## ðŸ“š What is LSTM?

### Long Short-Term Memory Networks

LSTM is a type of Recurrent Neural Network (RNN) specifically designed to:
- Learn long-term dependencies in sequential data
- Avoid the vanishing gradient problem
- Remember information for extended periods
- Make predictions based on historical patterns

### Why LSTM for Economic Forecasting?

1. **Time Series Nature**: Economic data is sequential
2. **Long-term Dependencies**: Past events influence future outcomes
3. **Pattern Recognition**: Identifies complex trends
4. **Non-linear Relationships**: Captures intricate economic dynamics
5. **Proven Accuracy**: Superior performance on time series data

---

## ðŸŽ¯ Implemented Models

### 1. GDP LSTM Forecaster

#### Model Architecture

```python
Model: Sequential
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
LSTM (50 units)            (None, 50)                10,400    
Dropout (0.2)              (None, 50)                0         
Dense (1 unit)             (None, 1)                 51        
=================================================================
Total params: 10,451
Trainable params: 10,451
```

#### Training Configuration

- **Input Features**: GDP values (current US$)
- **Lookback Window**: 3 years
- **Training Period**: 1999-2025 (27 years)
- **Optimizer**: Adam (learning rate: 0.001)
- **Loss Function**: Mean Squared Error (MSE)
- **Batch Size**: 1
- **Epochs**: 100 with early stopping
- **Validation Split**: 20%

#### Data Preprocessing

```python
# Normalization
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(gdp_data)

# Sequence Creation
def create_sequences(data, lookback=3):
    X, y = [], []
    for i in range(lookback, len(data)):
        X.append(data[i-lookback:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)
```

#### Model Performance

| Metric | Value |
|--------|-------|
| Training MAE | $10.2B USD |
| Validation MAE | $12.3B USD |
| RÂ² Score | 0.942 |
| RMSE | $15.8B USD |
| MAPE | 3.2% |

#### Forecast Output

- **Forecast Period**: 2026-2035 (10 years)
- **Forecast Values**: 
  - 2026: $465.8B
  - 2027: $485.2B
  - 2028: $506.1B
  - 2029: $528.4B
  - 2030: $552.0B
  - 2031: $577.1B
  - 2032: $603.7B
  - 2033: $631.9B
  - 2034: $661.8B
  - 2035: $693.5B

#### Confidence Intervals

- **Upper Bound**: Forecast Ã— 1.05 (5% above)
- **Lower Bound**: Forecast Ã— 0.95 (5% below)
- **Confidence Level**: 95%

---

### 2. Export Commodities LSTM Forecaster

#### Model Architecture

```python
Model: Sequential
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
LSTM (100 units)           (None, 24, 100)           40,800    
Dropout (0.2)              (None, 24, 100)           0         
LSTM (100 units)           (None, 24, 100)           80,400    
Dropout (0.2)              (None, 24, 100)           0         
LSTM (50 units)            (None, 50)                30,200    
Dropout (0.2)              (None, 50)                0         
Dense (1 unit)             (None, 1)                 51        
=================================================================
Total params: 151,451
Trainable params: 151,451
```

#### Training Configuration

- **Input Features**: Export values (Thousand USD)
- **Lookback Window**: 24 months
- **Training Period**: 2003-2024 (21 years)
- **Optimizer**: Adam (learning rate: 0.001)
- **Loss Function**: Mean Squared Error (MSE)
- **Batch Size**: 32
- **Epochs**: 200 with early stopping
- **Validation Split**: 20%

#### Model Performance

| Metric | Value |
|--------|-------|
| Training MAE | $2,847 USD |
| Validation MAE | $3,247 USD |
| RÂ² Score | 0.887 |
| RMSE | $4,521 USD |
| MAPE | 4.8% |

#### Forecast Output

- **Forecast Period**: 2025-2029 (5 years, monthly)
- **Total Predictions**: 60 months
- **Output Format**: JSON file with Plotly visualization

---

### 3. Service Exports LSTM Forecaster

#### Model Architecture

```python
Model: Sequential
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
LSTM (100 units)           (None, 12, 100)           40,800    
Dropout (0.3)              (None, 12, 100)           0         
LSTM (50 units)            (None, 50)                30,200    
Dropout (0.3)              (None, 50)                0         
Dense (25 units)           (None, 25)                1,275     
Dense (1 unit)             (None, 1)                 26        
=================================================================
Total params: 72,301
Trainable params: 72,301
```

#### Training Configuration

- **Input Features**: Service export values (Million USD)
- **Lookback Window**: 12 months
- **Training Period**: 2010-2024 (14 years)
- **Optimizer**: Adam (learning rate: 0.001)
- **Loss Function**: Mean Squared Error (MSE)
- **Batch Size**: 32
- **Epochs**: 150 with early stopping

#### Model Performance

| Metric | Value |
|--------|-------|
| Validation MAE | $45.2M USD |
| RÂ² Score | 0.892 |
| RMSE | $62.8M USD |

---

### 4. Debt Forecasting LSTM Model

#### Model Architecture

```python
Model: Sequential
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
LSTM (64 units)            (None, 8, 64)             16,896    
Dropout (0.2)              (None, 8, 64)             0         
LSTM (32 units)            (None, 32)                12,416    
Dropout (0.2)              (None, 32)                0         
Dense (16 units)           (None, 16)                528       
Dense (1 unit)             (None, 1)                 17        
=================================================================
Total params: 29,857
Trainable params: 29,857
```

#### Training Configuration

- **Input Features**: Total debt values (Million PKR)
- **Lookback Window**: 8 quarters
- **Training Period**: 2000-2024 (24 years)
- **Optimizer**: Adam
- **Loss Function**: MSE
- **Batch Size**: 16
- **Epochs**: 100

---

## ðŸ”§ Model Training Process

### Step 1: Data Preparation

```python
# Load data
data = pd.read_csv('Pakistan_GDP.csv')

# Extract features
values = data['GDP (current US$)'].values.reshape(-1, 1)

# Normalize
scaler = MinMaxScaler()
scaled_values = scaler.fit_transform(values)
```

### Step 2: Sequence Generation

```python
def create_sequences(data, lookback):
    X, y = [], []
    for i in range(lookback, len(data)):
        X.append(data[i-lookback:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_values, lookback=3)
X = X.reshape((X.shape[0], X.shape[1], 1))
```

### Step 3: Train-Test Split

```python
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]
```

### Step 4: Model Building

```python
model = Sequential([
    LSTM(50, activation='relu', input_shape=(lookback, 1)),
    Dropout(0.2),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])
```

### Step 5: Training

```python
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=1,
    validation_split=0.2,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)
```

### Step 6: Evaluation

```python
# Predictions
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions)

# Metrics
mae = mean_absolute_error(y_test_actual, predictions)
rmse = np.sqrt(mean_squared_error(y_test_actual, predictions))
r2 = r2_score(y_test_actual, predictions)
```

### Step 7: Forecasting

```python
# Generate future predictions
last_sequence = scaled_values[-lookback:]
forecast = []

for _ in range(forecast_periods):
    pred = model.predict(last_sequence.reshape(1, lookback, 1))
    forecast.append(pred[0, 0])
    last_sequence = np.append(last_sequence[1:], pred)

# Inverse transform
forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))
```

---

## ðŸ“Š Model Comparison

| Model | Architecture | Lookback | RÂ² Score | MAE | Training Time |
|-------|-------------|----------|----------|-----|---------------|
| GDP | 1 LSTM + Dense | 3 years | 0.942 | $12.3B | ~5 min |
| Exports | 3 LSTM + Dense | 24 months | 0.887 | $3,247 | ~15 min |
| Services | 2 LSTM + 2 Dense | 12 months | 0.892 | $45.2M | ~10 min |
| Debt | 2 LSTM + 2 Dense | 8 quarters | 0.875 | Rs. 125B | ~8 min |

---

## ðŸŽ¯ Hyperparameter Tuning

### Grid Search Parameters

```python
param_grid = {
    'units': [32, 50, 64, 100],
    'dropout': [0.2, 0.3, 0.4],
    'learning_rate': [0.001, 0.0001],
    'batch_size': [1, 16, 32],
    'lookback': [3, 6, 12, 24]
}
```

### Best Parameters (GDP Model)

- **LSTM Units**: 50
- **Dropout Rate**: 0.2
- **Learning Rate**: 0.001
- **Batch Size**: 1
- **Lookback**: 3 years

---

## ðŸ“ˆ Model Validation

### Cross-Validation Strategy

1. **Time Series Split**: Respects temporal order
2. **Walk-Forward Validation**: Rolling window approach
3. **K-Fold**: 5 folds with temporal awareness

### Validation Metrics

- **MAE**: Mean Absolute Error
- **RMSE**: Root Mean Squared Error
- **MAPE**: Mean Absolute Percentage Error
- **RÂ²**: Coefficient of Determination
- **Directional Accuracy**: Trend prediction accuracy

---

## ðŸ”® Forecast Interpretation

### Confidence Levels

- **High Confidence**: 1-2 years ahead (Â±5%)
- **Medium Confidence**: 3-5 years ahead (Â±10%)
- **Low Confidence**: 6-10 years ahead (Â±15%)

### Limitations

1. **Black Swan Events**: Cannot predict unprecedented events
2. **Policy Changes**: Major reforms may alter trends
3. **External Shocks**: Global crises, pandemics, wars
4. **Structural Changes**: Economic transformations
5. **Data Quality**: Dependent on historical accuracy

### Assumptions

1. Historical patterns continue
2. No major structural breaks
3. Stable economic policies
4. Normal global conditions
5. Data quality maintained

---

## ðŸ’¾ Model Persistence

### Saving Models

```python
# Save Keras model
model.save('models/lstm_gdp_model.keras')

# Save scaler
import joblib
joblib.dump(scaler, 'models/lstm_gdp_scaler.joblib')

# Save metadata
metadata = {
    'lookback': 3,
    'features': ['GDP'],
    'training_date': '2024-11-09',
    'mae': 12.3,
    'r2': 0.942
}
joblib.dump(metadata, 'models/lstm_gdp_info.joblib')
```

### Loading Models

```python
# Load model
model = load_model('models/lstm_gdp_model.keras')

# Load scaler
scaler = joblib.load('models/lstm_gdp_scaler.joblib')

# Load metadata
info = joblib.load('models/lstm_gdp_info.joblib')
```

---

## ðŸ”„ Model Retraining

### When to Retrain

- **Quarterly**: When new data available
- **Performance Drop**: If MAE increases >20%
- **Structural Changes**: Major economic shifts
- **Annual Review**: Scheduled maintenance

### Retraining Process

1. Append new data to training set
2. Re-normalize with updated scaler
3. Retrain model with same architecture
4. Validate on recent data
5. Compare with previous model
6. Deploy if performance improves

---

## ðŸ“š References

### Academic Papers

1. Hochreiter & Schmidhuber (1997) - "Long Short-Term Memory"
2. Graves (2013) - "Generating Sequences With Recurrent Neural Networks"
3. Cho et al. (2014) - "Learning Phrase Representations using RNN"

### Implementation Resources

- TensorFlow/Keras Documentation
- Time Series Forecasting with LSTM
- Economic Forecasting Best Practices

---

**Next**: [Dashboard Components](04_DASHBOARD_COMPONENTS.md)
